<!DOCTYPE html>
<html lang="en" prefix="og: http://ogp.me/ns#">
<head>
	<meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<title>

Kullback–Leibler divergence • || y ||

</title>
<meta name="description" content="KL divergence has a significant relationship with machine learning. I first encountered the ML - KL divergence relationship while studying variational inference in Probabilistic Graphical Models class. Since then I've encountered this premetric in every machine learning paper that has its basis in variational inference theory. Let's take a walk through this simple measure in its distilled form. ynorm, YNORM, ||y||, |y| is Pankesh Bamotra's personal blog on machine learning, statistics, and ramblings about Python programming.">
<meta name="google-site-verification" content="gOgaM4dnfNijpSpdApTBA1oNkmE47-F0GUNZR8lonu4" />
<link rel="alternate" type="application/rss+xml" title="RSS 2.0" href="/feed.xml">

<!-- icons -->
<link rel="apple-touch-icon" href="/apple-touch-icon.png">
<link rel="icon" type="image/png" href="/favicon-96x96.png" sizes="96x96">
<link rel="icon" type="image/png" href="/favicon-32x32.png" sizes="32x32">
<link rel="icon" type="image/png" href="/android-chrome-192x192.png" sizes="192x192">
<meta name="theme-color" content="#00f0ff">
<meta name="msapplication-TileColor" content="#00f0ff">
<meta name="msapplication-TileImage" content="/mstile-310x310.png">
<!-- /icons -->

<!-- og tags -->
<meta property="og:site_name" content="|| y ||">
<meta property="og:title" content="Kullback–Leibler divergence • || y ||">
<meta property="og:locale" content="en">

  
    <meta property="og:image" content="http://ynorm.com/img/kldivergence.jpg">
  

<meta property="og:type" content="website">
<meta property="og:url" content="http://ynorm.com/2017/10/12/KL-divergence-simplified/">
<meta property="og:description" content="

KL divergence has a significant relationship with machine learning. I first encountered the ML - KL divergence relationship while studying variational inference in Probabilistic Graphical Models class. Since then I've encountered this premetric in every machine learning paper that has its basis in variational inference theory. Let's take a walk through this simple measure in its distilled form.
">
<!-- /og tags -->

<script src="https://ajax.googleapis.com/ajax/libs/jquery/2.1.4/jquery.min.js"></script>
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-49727693-10"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-49727693-10');
</script>

<script src="/js/modernizr.js"></script>
<script src="/js/fitvids.js"></script>
<link href='//fonts.googleapis.com/css?family=Source+Code+Pro:400,700|Source+Sans+Pro:600,900|Crimson+Text:700italic,600,600italic,400,700,400italic' rel='stylesheet' type='text/css'>
<link rel="stylesheet" href="/css/reset.css">
<link rel="stylesheet" href="/css/pygments.css">

<link rel="stylesheet" href="/css/style.css">
<!--[if gte IE 9]>
  <style type="text/css">
    .gradient {
       filter: none;
    }
  </style>
<![endif]-->


<style>
header {
  margin-bottom: 40px;
}
@media all and (min-width: 500px) {
  header {
    margin-bottom: 100px;
  }
}
header::after {
  display: none;
}
</style>


</head>
<body>
	
	<div class="cover-image-container">
		<img alt="Cover image" src="/img/covers/kldivergence.jpg">
	</div>
	
	<div class="wrap">
		<header>
  <a href="/" class="website-title">
    
      
        &#x2190; Home
      
    
  </a>
</header>

		


<article>
  <span class="meta">Oct 12, 2017 • 5&nbsp;min read</span>
	<h1>Kullback–Leibler divergence</h1>
		<p>KL divergence is a <a href="https://en.wikipedia.org/wiki/Metric_(mathematics)#Premetrics">premetric</a> that finds its root in information theory. It has a close relationship with <a href="https://en.wiktionary.org/wiki/Shannon_entropy">Shannon entropy</a> and we’ll walk through this relationship in the subsequent discussion. In its most basic sense, KL divergence measures the proximity between distributions. When we talk about KL divergence between two distribution say P and Q, it’s denoted as</p>

<script type="math/tex; mode=display">D_{KL} \left(P  \Vert  Q\right)</script>

<h3 id="mathematical-background">Mathematical background</h3>

<p>KL divergence belongs to a class of divergence measures known as <i>f-divergence</i>. For distributions P and Q and  a convex function f(t) defined over t &gt; 0 with f(1) = 0 is given by</p>

<script type="math/tex; mode=display">D_{f} \left(P  \Vert Q\right) = \sum_{t} Q(t) f\Big(\frac{P(t)}{Q(t)}\Big)</script>

<p>To derive KL divergence we set f(t) = t log t. For P(t) = Q(t) = 0, f-divergence is taken as zero. As per literature, KL divergence <script type="math/tex">D_{KL} \left(P  \middle\|  Q\right)</script> requires P to be <i>absolute continuous</i>. Mathematically, this would mean KL divergence is undefined when for any t, P(t) <script type="math/tex">\neq</script> 0 but Q(t) = 0. An intuitive explanation for this will be presented later.</p>

<p>Three important properties of KL divergence are:-</p>
<ul>
  <li><script type="math/tex">D_{KL} \left(P  \Vert  Q\right) \geq 0</script>. The equality happens when P = Q everywhere. This is known as Gibb’s inequality.</li>
  <li>In general, <script type="math/tex">D_{KL} \left(P  \Vert  Q\right) \neq D_{KL} \left(Q  \Vert  P\right)</script>. That means KL divergence is not symmetric and hence is not a metric/distance measure.</li>
  <li>KL divergence doesn’t obey triangle inequality.</li>
</ul>

<h3 id="shannon-entropy">Shannon entropy</h3>

<p>In computer science theory, entropy is one of the most studied topics. Thanks to Claude Shannon who gave us Shannon entropy. For a random variable X with PMF P(X), Shannon entropy is defined as</p>

<script type="math/tex; mode=display">H(X) = - \sum_{x}P(x)\hspace{0.2cm}log_{2}\left(\hspace{0.1cm}P(x)\right)</script>

<p>Intuitively, entropy gives us the lower bound on the number of bits required to optimally encode each observation of x <sup id="fnref:1"><a href="#fn:1" class="footnote">1</a></sup>. However, it must be kept in mind that we don’t get to know what the optimal encoding is! The choice of use logarithm base 2 comes from information theory literature leading to entropy’s unit as bits.</p>

<h3 id="kl-divergence-and-its-relationship-with-entropy">KL divergence and its relationship with entropy</h3>

<p>We saw that KL divergence is defined as <script type="math/tex">D_{KL} \left(P  \Vert  Q\right) = \sum_{x} P(x) log\Big(\frac{P(x)}{Q(x)}\Big)</script>. Let’s rewrite this by expanding the log term. We get,</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}
D_{KL} \left(P  \Vert Q\right) &= \sum_{x} P(x) log\Big(\frac{P(x)}{Q(x)}\Big) \\
&= \sum_{x} P(x) log(P(x)) - \sum_{x} P(x) log(Q(x)) \\
&= -H(X) + H(P, Q)
\end{align} %]]></script>

<p>The two terms in the final step are well known. H(X) is the Shannon entropy which we described in the previous section. H(P, Q) is, yeah you probably guessed it, cross-entropy. Using Gibbs inequality, we can say that cross entropy is always greater than or equal to the corresponding Shannon entropy.</p>

<p>Now, we describe KL divergence in terms of Shannon entropy and cross-entropy. Shannon entropy as we said above is the minimum number of bits required to optimally encode a distribution. Cross-entropy H(P, Q) on the other hand is the number of bits required to encode distribution P using an encoding that’s optimal for distribution Q but not for P. Consequently, KL divergence is the expected number of extra bits that are used under this sub-optimal encoding.</p>

<p>Let’s revisit the discussion on why we require P to be <i>absolute continuous</i>. Having Q(x) = 0 when <script type="math/tex">P(x) \neq 0</script> would mean that we’re trying to approximate a <i>probable</i> event with something that’s definitely not going to happen. So, when such an event happens (in distribution P), KL divergence would essentially diverge logarithmically. In other words, the sub-optimal encoding has no way to encode such an event! So, KL divergence is undefined in such a case.</p>

<h3 id="treading-to-machine-learning-domain">Treading to machine learning domain</h3>

<p>In most of the ML algorithms, we resort to optimising cross entropy and not KL divergence because the Shannon entropy term is independent of the model parameters and acts like a constant when taking derivative of log-likelihood. In fact, it can be shown that minimizing KL divergence is equivalent to minimizing negative log-likelihood.</p>

<p>Let P = <script type="math/tex">p\left(x \vert \theta^{*}\right)</script> be the true data distribution and model distribution be Q = <script type="math/tex">p\left(x \vert \theta \right)</script>. Then by definition of KL divergence,</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}
D_{KL}[P(x \vert \theta^*) \, \Vert \, P(x \vert \theta)] &= \mathbb{E}_{x \sim P(x \vert \theta^*)}\left[\log \frac{P(x \vert \theta^*)}{P(x \vert \theta)} \right] \\
        &= \mathbb{E}_{x \sim P(x \vert \theta^*)}\left[\log \, P(x \vert \theta^*) - \log \, P(x \vert \theta) \right] \\
        &= H(X) - \mathbb{E}_{x \sim P(x \vert \theta^*)}\left[\log \, P(x \vert \theta) \right]
\end{align} %]]></script>

<p>For a large number of samples drawn from the true distribution we have <script type="math/tex">\frac{1}{N} \sum_x \log \, P(x \vert \theta) = \mathbb{E}_{x \sim P(x \vert \theta^*)}\left[\log \, P(x \vert \theta) \right]</script> using the law of large numbers. Left-hand side in the equation represents log-likelihood of data samples. Comparing this result with the derivation above we can conclude that minimizing KL divergence is equivalent to minimizing negative log-likelihood.</p>

<p>These results have been used in variational inference theory and the most recent examples are Variational Autoencoders. The discussion about VAEs is reserved for another post. But you can read about them in this <a href="https://arxiv.org/pdf/1606.05908.pdf">Tutorial on Variational Autoencoders</a> by <a href="http://www.carldoersch.com/">Carl Doersch</a>.</p>

<p><sub>Cover credit: <a href="https://www.flickr.com/photos/shonk/7537733822/">shonk</a> via <a href="https://visualhunt.com/re/59119f">Visual Hunt</a> / <a href="http://creativecommons.org/licenses/by-nc-nd/2.0/"> CC BY-NC-ND</a></sub></p>
<div class="footnotes">
  <ol>
    <li id="fn:1">
      <p><a href="http://www.cs.cmu.edu/~venkatg/teaching/ITCS-spr2013/notes/15359-2009-lecture25.pdf" target="_blank">CMU 15-359: Elements of Information Theory</a>&nbsp;<a href="#fnref:1" class="reversefootnote">&#8617;</a></p>
    </li>
  </ol>
</div>

  
    
      
        <small><em>Post by: Pankesh Bamotra </em></small>
      
    
  
</article>




<div id="disqus_thread"></div>
<script type="text/javascript">
    /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
    var disqus_shortname = 'ynorm'; // Required - Replace '<example>' with your forum shortname

    /* * * DON'T EDIT BELOW THIS LINE * * */
    (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>


	  <script>
	    $("article").fitVids();
	    $('article p').each(function(i){
				if (($(this).find('img').length) && (!$.trim($(this).text()).length))  {
					$(this).addClass('img-only');
				}
				if ($.trim($(this).text()).length - $.trim($(this).find('small').text()).length == 0 && ($(this).find('img').length)){
					$(this).addClass('img-only-source');
				}
	  	});
	  </script>
	</div>
	<footer>
  <div class="inner">
    <p>Pankesh Bamotra • 2017</p>
  </div>
</footer>

</body>
</html>
